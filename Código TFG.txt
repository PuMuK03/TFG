import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, ConfusionMatrixDisplay
)
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier


df = pd.read_excel("Datos_TFG_limpios.xlsx")


umbral_binario = 45
df["RecommendBinary"] = df["Probability for the product to be recommended to the person"].apply(
    lambda x: 1 if x > umbral_binario else 0
)


features = [
    'Customer Loyalty Score',
    'Customer review sentiment score (overall)',
    'Price of the product in euros',
    'Age of Customer',
    'Number of clicks on similar products',
    'Average rating given to similar products',
    'Rating of the product',
    'Number of similar products purchased so far',
    'Return History'
]
X = df[features].copy()
y = df["RecommendBinary"]


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

lr_model = LogisticRegression(random_state=42, max_iter=1000)
lr_model.fit(X_train_scaled, y_train)
probs_lr = lr_model.predict_proba(X_test_scaled)[:, 1]

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train_scaled, y_train)
probs_rf = rf_model.predict_proba(X_test_scaled)[:, 1]

xgb_model = XGBClassifier(n_estimators=100, random_state=42, use_label_encoder=False, eval_metric="logloss")
xgb_model.fit(X_train_scaled, y_train)
probs_xgb = xgb_model.predict_proba(X_test_scaled)[:, 1]


def evaluar_modelo(probs, y_true, nombre_modelo):
    thresholds = np.arange(0.1, 0.9, 0.05)
    metrics = []

    for t in thresholds:
        y_pred = (probs >= t).astype(int)
        acc = accuracy_score(y_true, y_pred)
        prec = precision_score(y_true, y_pred, zero_division=0)
        rec = recall_score(y_true, y_pred)
        f1 = f1_score(y_true, y_pred)
        metrics.append((t, acc, prec, rec, f1))

    metrics_df = pd.DataFrame(metrics, columns=['Umbral', 'Accuracy', 'Precision', 'Recall', 'F1'])
    metrics_df.set_index('Umbral').plot(figsize=(10,6), marker='o')
    plt.title(f"Métricas vs. Umbral de decisión ({nombre_modelo})")
    plt.ylabel("Score")
    plt.grid(True)
    plt.show()

    return metrics_df


metrics_lr = evaluar_modelo(probs_lr, y_test, "Regresión Logística")
metrics_rf = evaluar_modelo(probs_rf, y_test, "Random Forest")
metrics_xgb = evaluar_modelo(probs_xgb, y_test, "XGBoost")


umbral_pred = 0.35
for modelo, probs, nombre in [
    (lr_model, probs_lr, "Regresión Logística"),
    (rf_model, probs_rf, "Random Forest"),
    (xgb_model, probs_xgb, "XGBoost")
]:
    y_pred = (probs >= umbral_pred).astype(int)
    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred, zero_division=0)
    rec = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    print(f"\n=== {nombre.upper()} (Umbral ajustado a {umbral_pred}) ===")
    print(f"Accuracy:  {acc:.3f}")
    print(f"Precision: {prec:.3f}")
    print(f"Recall:    {rec:.3f}")
    print(f"F1-Score:  {f1:.3f}")

    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["No recomendado", "Recomendado"])
    disp.plot(cmap='viridis')
    plt.title(f"Matriz de Confusión - {nombre} (umbral {umbral_pred})")
    plt.show()



--------------------


fig, axs = plt.subplots(2, 2, figsize=(14, 10))
fig.suptitle("Comparación de métricas entre modelos", fontsize=16)


axs[0, 0].plot(metrics_lr['Umbral'], metrics_lr['Accuracy'], label='Regresión Logística', marker='o')
axs[0, 0].plot(metrics_rf['Umbral'], metrics_rf['Accuracy'], label='Random Forest', marker='o')
axs[0, 0].plot(metrics_xgb['Umbral'], metrics_xgb['Accuracy'], label='XGBoost', marker='o')
axs[0, 0].set_title("Accuracy vs. Umbral")
axs[0, 0].set_xlabel("Umbral")
axs[0, 0].set_ylabel("Accuracy")
axs[0, 0].legend()
axs[0, 0].grid(True)

axs[0, 1].plot(metrics_lr['Umbral'], metrics_lr['Precision'], label='Regresión Logística', marker='o')
axs[0, 1].plot(metrics_rf['Umbral'], metrics_rf['Precision'], label='Random Forest', marker='o')
axs[0, 1].plot(metrics_xgb['Umbral'], metrics_xgb['Precision'], label='XGBoost', marker='o')
axs[0, 1].set_title("Precision vs. Umbral")
axs[0, 1].set_xlabel("Umbral")
axs[0, 1].set_ylabel("Precision")
axs[0, 1].legend()
axs[0, 1].grid(True)


axs[1, 0].plot(metrics_lr['Umbral'], metrics_lr['Recall'], label='Regresión Logística', marker='o')
axs[1, 0].plot(metrics_rf['Umbral'], metrics_rf['Recall'], label='Random Forest', marker='o')
axs[1, 0].plot(metrics_xgb['Umbral'], metrics_xgb['Recall'], label='XGBoost', marker='o')
axs[1, 0].set_title("Recall vs. Umbral")
axs[1, 0].set_xlabel("Umbral")
axs[1, 0].set_ylabel("Recall")
axs[1, 0].legend()
axs[1, 0].grid(True)


axs[1, 1].plot(metrics_lr['Umbral'], metrics_lr['F1'], label='Regresión Logística', marker='o')
axs[1, 1].plot(metrics_rf['Umbral'], metrics_rf['F1'], label='Random Forest', marker='o')
axs[1, 1].plot(metrics_xgb['Umbral'], metrics_xgb['F1'], label='XGBoost', marker='o')
axs[1, 1].set_title("F1 vs. Umbral")
axs[1, 1].set_xlabel("Umbral")
axs[1, 1].set_ylabel("F1")
axs[1, 1].legend()
axs[1, 1].grid(True)

plt.tight_layout(rect=[0, 0, 1, 0.95])
plt.show()


------------------

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd


plt.figure(figsize=(10, 10))

plt.subplot(2, 1, 1)
sns.histplot(df["Probability for the product to be recommended to the person"], bins=30, kde=True, color="skyblue")
plt.title("Distribución de la Probabilidad Original")
plt.xlabel("Probabilidad de Recomendación")
plt.ylabel("Cantidad de Productos")

plt.subplot(2, 1, 2)
sns.countplot(x="RecommendBinary", data=df)
plt.title("Distribución de clases 0 y 1")
plt.xlabel("Clase")
plt.ylabel("Count")

plt.tight_layout()
plt.show()


--------------------------------

import matplotlib.pyplot as plt
import seaborn as sns


features = X.columns.tolist()


importance_lr = pd.Series(abs(lr_model.coef_[0]), index=features).sort_values(ascending=False)


importance_rf = pd.Series(rf_model.feature_importances_, index=features).sort_values(ascending=False)


importance_xgb = pd.Series(xgb_model.feature_importances_, index=features).sort_values(ascending=False)


fig, axes = plt.subplots(1, 3, figsize=(18, 6))

sns.barplot(x=importance_lr.values, y=importance_lr.index, ax=axes[0], palette="Blues_r")
axes[0].set_title("Importancia de variables - Regresión Logística")

sns.barplot(x=importance_rf.values, y=importance_rf.index, ax=axes[1], palette="Greens_r")
axes[1].set_title("Importancia de variables - Random Forest")

sns.barplot(x=importance_xgb.values, y=importance_xgb.index, ax=axes[2], palette="Oranges_r")
axes[2].set_title("Importancia de variables - XGBoost")

plt.tight_layout()
plt.show()
